# -*- coding: utf-8 -*-
"""Untitled59.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_3FxDc5HAmePKeOtAdQyM15poa0-gktW
"""

# -*- coding: utf-8 -*-


import torch # Importing the PyTorch library
import torch.nn as nn # Importing the neural network module from PyTorch
# Checking if CUDA (GPU) is available, and setting the device accordingly
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# imports
# For generating random numbers: importing random
# For deep learning framework: importing torch
# For data manipulation and analysis: importing pandas as pd
# For displaying progress bars: importing tqdm
# For mathematical operations: importing math
# For building neural networks: importing torch.nn as nn
# For plotting graphs: importing matplotlib.pyplot as plt
# For optimization algorithms: importing torch.optim as optim
# For computer vision tasks: importing torchvision
# For numerical computations: importing numpy as np
# For handling file paths: importing pathlib
# For image transformations: importing torchvision.transforms as transforms
# For additional neural network functions: importing torch.nn.functional as F
# For interacting with the operating system: importing os
# For accessing standard datasets: importing torchvision.datasets as datasets

import wandb
import argparse
import random
import torch
from tqdm import tqdm
import pandas as pd
import math
import torch.nn as nn
import matplotlib.pyplot as plt
import torch.optim as optim
import torchvision
import numpy as np
import pathlib
import torchvision.transforms as transforms
import torch.nn.functional as F
from torch import optim
import os
from torch import nn
import torchvision.datasets as datasets
from torch.utils.data import (
    DataLoader, random_split
)
from torchvision.datasets import ImageFolder

# Setting PYTHONHASHSEED environment variable to control hash randomization: setting os.environ["PYTHONHASHSEED"] to "1"
# Setting the random seed for Python: seeding random number generator with seed 1
# Setting the random seed for CUDA devices: seeding CUDA random number generator with seed 1
# Setting the random seed for all CUDA devices: seeding random number generators for all CUDA devices with seed 1
# Setting the random seed for numpy: seeding numpy random number generator with seed 1
# Setting the random seed for PyTorch: seeding PyTorch random number generator with seed 1
# Setting PyTorch to use deterministic algorithms for cuDNN: setting torch.backends.cudnn.deterministic to True
# Disabling cuDNN benchmark mode to ensure deterministic computation: setting torch.backends.cudnn.benchmark to False
os.environ["PYTHONHASHSEED"] = str(1)
random.seed(1)
torch.cuda.manual_seed(1)
torch.cuda.manual_seed_all(1)
np.random.seed(1)
torch.manual_seed(1)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False


# For data manipulation and analysis
# For deep learning framework
# For building neural networks
# For data loading and handling
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

'''
The class Vocabulary is employed to generate Word_Vocab from the training dataset.
'''
class Word_Vocab:
    """
    Parameters:
      trg_lang (string): The name of the target language.
      src_lang (string): The name of the source language.
      file_path (string): The path to the CSV file containing the training dataset.

    Raises:
      ValueError: If the specified file_path does not exist.


    """
    def __init__(self, file_path, src_lang, trg_lang):
        # Class constructor to initialize the translation dataset
        # Parameters:
        #   - file_path: Path to the CSV file containing translations
        #   - src_lang: Source language column name in the CSV file
        #   - trg_lang: Target language column name in the CSV file
        # Read the CSV file into a Pandas DataFrame.
        def get_translations():
          return pd.read_csv(file_path, header=None, names=[src_lang, trg_lang])
        self.translations = get_translations()
        # It will drop any rows with missing values
        self.translations.dropna()
        def enumeration_across_trg():
           return {char: i+3 for i, char in enumerate(sorted(list(set(''.join(self.translations[trg_lang].tolist())))))}
        self.src_lang = src_lang
        def enumeration_across_src():
            return {char: i+3 for i, char in enumerate(sorted(list(set(''.join(self.translations[src_lang].tolist())))))}
        self.trg_lang = trg_lang
        # Create a dictionary that maps each character in the source language to an integer index.
        self.trg_vocab = enumeration_across_trg()
        # Create a dictionary that maps each character in the target language to an integer index.
        self.src_vocab = enumeration_across_src()

        def set_0():
          return 0
        # Add special tokens to the vocabularies.
        self.trg_vocab['<'] = set_0()
        self.src_vocab['<'] = set_0()
        def set_1():
            return 1
        def set_2():
            return 2
        self.trg_vocab['<unk>'] = set_2()
        self.src_vocab['<pad>'] = set_1()
        self.trg_vocab['<pad>'] = set_1()

        self.src_vocab['<unk>'] = set_2()

        # Extract the unique characters in the source and target languages
        src_chars = sorted(set(''.join(self.translations[src_lang])))
        trg_chars = sorted(set(''.join(self.translations[trg_lang])))

        def get_char_to_idx1():
          return {char: idx+3 for idx, char in enumerate(trg_chars)}
        # Assign an index to each character in the source and target languages
        self.t_char_to_idx = get_char_to_idx1()
        self.t_char_to_idx['<unk>']=2
        self.t_idx_to_char = {idx: char for char, idx in self.t_char_to_idx.items()}
        def get_char_to_idx2():
            return {char: idx+3 for idx, char in enumerate(src_chars)}
        self.s_char_to_idx = get_char_to_idx2()
        self.s_char_to_idx['<unk>']=2

        self.s_idx_to_char = {idx: char for char, idx in self.s_char_to_idx.items()}


    def utitlity_3(x,y):
        if(x>y):
          return 1
        else:
          return 0
    def ret_all_vocab(self):
           return self.src_vocab,self.trg_vocab,self.t_char_to_idx,self.t_idx_to_char,self.s_char_to_idx,self.s_idx_to_char
    def get(self):
         # This function returns the source and target vocabularies, as well as the dictionaries that map characters to integer indexes and vice versa.
        return self.ret_all_vocab()



class TransliterationDataset(Dataset):
    """
   Function Parameters:
    - src_lang (string): Specifies the source language from which translation originates.
    - trg_lang (string): Specifies the target language into which translation is done.
    - trg_vocab (Word_Vocab): Refers to the vocabulary tailored for the target language.
    - file_path (string): Indicates the precise location of the CSV file containing the training data.
    - src_vocab (Word_Vocab): Refers to the vocabulary customized for the source language.
    Raises:
     - ValueError: Raised if the provided file_path does not exist.

    """
    def __init__(self, file_path, src_lang, trg_lang,src_vocab,trg_vocab,t_char_to_idx):
        """
        Initializes the TransliterationDataset.

        Parameters:
            - file_path (string): Path to the CSV file containing translations.
            - src_lang (string): Source language column name in the CSV file.
            - trg_lang (string): Target language column name in the CSV file.
            - src_vocab (Word_Vocab): Vocabulary customized for the source language.
            - trg_vocab (Word_Vocab): Vocabulary tailored for the target language.
            - t_char_to_idx (dict): Dictionary mapping characters to integer indexes for target language.
        """
        self.src_lang = src_lang
        def set_reading_csv():
          return pd.read_csv(file_path, header=None, names=[src_lang, trg_lang])
        def set_max_scr_len():
          return max([len(word) for word in self.translations[src_lang].tolist()])+1
        self.translations = set_reading_csv()
        self.translations.dropna()
        def set_trg_len():
          return max([len(word) for word in self.translations[trg_lang].tolist()])+1
        self.t_char_to_idx = t_char_to_idx
        self.trg_lang = trg_lang
        self.trg_vocab = trg_vocab
        self.src_vocab = src_vocab
        self.max_src_len = set_max_scr_len()

        self.max_trg_len = set_trg_len()

    def __len__(self):
        return len(self.translations)

    def __getitem__(self, idx):
        def set_trans_trg():
            return self.translations.iloc[idx][self.trg_lang]

        src_word = self.translations.iloc[idx][self.src_lang]
        def trg_vocab():
          return [self.trg_vocab.get(char, self.src_vocab['<unk>']) for char in trg_word]
        trg_word = set_trans_trg()
        # Initialize the start-of-word token
        sow=0

        # Convert source and target words to lists of Word_Vocab indices
        src = [self.src_vocab.get(char, self.src_vocab['<unk>']) for char in src_word]
        trg = trg_vocab()
        # Insert the start-of-word token at the beginning
        trg.insert(0, sow)
        def ret_len_tar():
            return len(trg);

        src.insert(0, sow)
        def ret_src_len():
            return len(src)
        def trg_pad_set():
          return [self.trg_vocab['<pad>']] * (self.max_trg_len - trg_len)

        trg_len = ret_len_tar()
        src_len = ret_src_len()


        # Pad the source and target sequences with the <pad> token
        src_pad = [self.src_vocab['<pad>']] * (self.max_src_len - src_len)
        trg_pad = trg_pad_set()
        # Extend the source and target sequences with padding
        src.extend(src_pad)
        trg.extend(trg_pad)
        def ret_trg_len():
          return torch.LongTensor(trg)
        # Convert source and target sequences to tensors
        src = torch.LongTensor(src)
        trg = ret_trg_len()

        return src, trg, src_len, trg_len


def data_loading(bs):
    '''
    This function is designed to load data into batches, with the batch size being specified as an argument.
    '''
    # Define the paths for the train, validation, and test CSV files
    def get_test_data_path():
      return "/content/drive/MyDrive/aksharantar_sampled/hin/hin_test.csv"
    def get_val_data_path():
      return "/content/drive/MyDrive/aksharantar_sampled/hin/hin_valid.csv"
    def get_train_data_path():
      return "/content/drive/MyDrive/aksharantar_sampled/hin/hin_train.csv"
    test_path  = get_test_data_path()
    val_path  = get_val_data_path()
    train_path  = get_train_data_path()
    vocab = Word_Vocab(train_path, 'src', 'trg')
    def set_data_p():
        return True
    src_vocab,trg_vocab,t_char_to_idx,t_idx_to_char,s_char_to_idx,s_idx_to_char=vocab.get()
    # Create data loaders
    test_loader = DataLoader(TransliterationDataset(test_path, 'src', 'trg',src_vocab,trg_vocab,t_char_to_idx), batch_size=bs, shuffle=False)
    val_loader =DataLoader(TransliterationDataset(val_path, 'src', 'trg',src_vocab,trg_vocab,t_char_to_idx), batch_size=bs, shuffle=False)
    train_loader = DataLoader(TransliterationDataset(train_path, 'src', 'trg',src_vocab,trg_vocab,t_char_to_idx), batch_size=bs, shuffle=True)
    set_data_p()
    return train_loader,test_loader,val_loader,t_idx_to_char,s_idx_to_char
train_loader,test_loader,val_loader,t_idx_to_char,s_idx_to_char=data_loading(32)

class Encoder(nn.Module):
    """
        Initializes the Encoder module.

        Parameters:
            - input_dim (int): Dimensionality of the input data.
            - embedded_size (int): Dimensionality of the embedding space.
            - hidden_dim (int): Dimensionality of the hidden state.
            - num_layers (int): Number of recurrent layers.
            - bidirectional (bool): Specifies if the encoder is bidirectional.
            - cell_type (str): Type of recurrent cell ('rnn', 'lstm', or 'gru').
            - dp (float): Dropout probability.
    """
    def __init__(self, input_dim, embedded_size,hidden_dim, num_layers,bidirectional, cell_type,dp):
        def utility_u1(x):
            return x>0
        super(Encoder, self).__init__()
        self.bidirectional=bidirectional
        self.input_dim = input_dim
        def ret_linear_layer1():
            return nn.Linear(hidden_dim * 2, hidden_dim)
        self.hidden_dim = hidden_dim
        self.cell_type = cell_type
        def ret_dropout():
            return nn.Dropout(dp)
        self.embedded_size=embedded_size
        self.num_layers = num_layers
        self.dropout = ret_dropout()
        def check_bid():
            if self.bidirectional:
                return 2
            else:
                return 1
        self.fc_h = ret_linear_layer1()
        self.fc_c = ret_linear_layer1()
        self.dir=check_bid()

        self.embedding = nn.Embedding(input_dim,embedded_size)
        def get_gru():
            return nn.GRU(embedded_size, hidden_dim, num_layers,bidirectional=bidirectional)
        def get_lstm():
            return nn.LSTM(embedded_size, hidden_dim, num_layers,bidirectional=bidirectional)
        def get_rnn():
            return nn.RNN(embedded_size, hidden_dim, num_layers,bidirectional=bidirectional)
        if cell_type == 'gru':
              self.rnn = get_gru()
        elif cell_type == 'lstm':
              self.rnn = get_lstm()
        elif cell_type == 'rnn':
              self.rnn = get_rnn()
        else:
            raise ValueError("Invalid cell type. Choose 'rnn', 'lstm', or 'gru'.")

    def forward(self, src):
        """
        Forward pass of the Seq2Seq model.

        Parameters:
            - src (Tensor): Input sequence tensor (source language).
            - trg (Tensor): Target sequence tensor (target language).
            - teacher_forcing_ratio (float): Probability of using teacher forcing during training.

        Returns:
            - outputs (Tensor): Output sequence tensor.
        """
        def get_emdd():
            return self.dropout(self.embedding(src))
        def get_hidden(hidden):
            return self.fc_h(torch.cat((hidden[0:1], hidden[1:2]), dim=2))
        embedded = get_emdd()
        if self.bidirectional:
            if self.cell_type != 'lstm':
                output, hidden = self.rnn(embedded)
                hidden = get_hidden(hidden)
                return output,hidden



            else:
                output, (hidden, cell) = self.rnn(embedded)
                hidden = get_hidden(hidden)
                def get_cell():
                    return self.fc_c(torch.cat((cell[0:1], cell[1:2]), dim=2))
                cell = get_cell()
                return output, (hidden, cell)


        else:
            if self.cell_type != 'lstm':
                output, hidden = self.rnn(embedded)
                return output,hidden

            else:
                output, (hidden, cell) = self.rnn(embedded)
                return output, (hidden, cell)


class Decoder(nn.Module):
    """
        Initializes the Decoder module.

        Parameters:
            - output_dim (int): Dimensionality of the output data.
            - embedded_size (int): Dimensionality of the embedding space.
            - hidden_dim (int): Dimensionality of the hidden state.
            - num_layers (int): Number of recurrent layers.
            - bidirectional (bool): Specifies if the decoder is bidirectional.
            - cell_type (str): Type of recurrent cell ('rnn', 'lstm', or 'gru').
            - dp (float): Dropout probability.
    """
    def __init__(self, output_dim,embedded_size, hidden_dim, num_layers,bidirectional,cell_type,dp):
        def utility(x):
          return x>1
        super(Decoder, self).__init__()
        self.bidirectional=bidirectional
        self.output_dim = output_dim
        def get_hidden_dim():
            return hidden_dim
        self.num_layers = num_layers
        self.cell_type = cell_type
        def get_dropout():
            return nn.Dropout(dp)
        self.embedded_size=embedded_size
        self.hidden_dim = get_hidden_dim()
        def check_bid():
            if self.bidirectional:
                return 2
            else:
                return 1
        self.dropout = get_dropout()
        self.dir=check_bid()
        self.embedding = nn.Embedding(output_dim,embedded_size)

        def get_gru():
            return nn.GRU((hidden_dim*self.dir)+embedded_size, hidden_dim, num_layers)
        def get_lstm():
            return nn.LSTM((hidden_dim*self.dir)+embedded_size, hidden_dim, num_layers)
        def get_rnn():
            return nn.RNN((hidden_dim*self.dir)+embedded_size, hidden_dim, num_layers)
        if cell_type == 'gru':
            self.rnn = get_gru()
        elif cell_type == 'lstm':
            self.rnn = get_lstm()
        elif cell_type == 'rnn':
            self.rnn = get_rnn()
        else:
            raise ValueError("Invalid cell type. Choose 'rnn', 'lstm', or 'gru'.")

        def get_energy():
            return nn.Linear((hidden_dim *(self.dir+1) ), 1)
        self.dropout = nn.Dropout(dp)
        def get_outfunc():
            return nn.Softmax(dim=0)
        self.relu = nn.ReLU()
        self.fc_out = nn.Linear(hidden_dim, output_dim)
        self.softmax = get_outfunc()
        self.energy = get_energy()

    def forward(self, input,encoder_states,hidden):
        """
        Forward pass of the Seq2Seq model.

        Parameters:
            - src (Tensor): Input sequence tensor (source language).
            - trg (Tensor): Target sequence tensor (target language).
            - teacher_forcing_ratio (float): Probability of using teacher forcing during training.

        Returns:
            - outputs (Tensor): Output sequence tensor.
        """
        def get_reshaped():
            return hidden[0].repeat(sequence_length,1,1)
        input = input.unsqueeze(0)
        embedded = self.dropout(self.embedding(input))
        sequence_length = encoder_states.shape[0]
        h_reshaped = get_reshaped()
        def get_energy():
            return self.relu(self.energy(torch.cat((h_reshaped,encoder_states), dim=2)))
        energy = get_energy()
        attention = self.softmax(energy)
        def get_context_v(attention,encoder_states):
            return torch.bmm(attention, encoder_states).permute(1,0,2)
        attention =attention.permute(1,2,0)

        encoder_states =encoder_states.permute(1,0,2)

        context_vector = get_context_v(attention,encoder_states)

        rnn_input = torch.cat((context_vector, embedded), dim=2)
        def get_fc_out(output):
            return self.fc_out(output)

        output, hidden = self.rnn(rnn_input, hidden)
        def squeezing(output):
            return output.squeeze(0)

        output = get_fc_out(output)
        output = squeezing(output)
        return output, hidden

class Seq2Seq(nn.Module):
    """
        Initializes the Seq2Seq model.

        Parameters:
            - encoder (Encoder): The encoder module.
            - decoder (Decoder): The decoder module.
            - cell_type (str): Type of recurrent cell ('rnn', 'lstm', or 'gru').
            - bidirectional (bool): Specifies if the encoder is bidirectional.
    """
    def __init__(self, encoder, decoder,cell_type,bidirectional):
        def utility(x):
            return x>1
        super(Seq2Seq, self).__init__()
        self.bidirectional=bidirectional
        self.encoder = encoder
        def get_cell_t():
            return cell_type
        def set_decoder():
            return decoder
        self.cell_type = get_cell_t()
        self.decoder = set_decoder()

    def forward(self, src, trg, teacher_forcing_ratio=0.5):
        """
        Forward pass of the Seq2Seq model.

        Parameters:
            - src (Tensor): Input sequence tensor (source language).
            - trg (Tensor): Target sequence tensor (target language).
            - teacher_forcing_ratio (float): Probability of using teacher forcing during training.

        Returns:
            - outputs (Tensor): Output sequence tensor.
        """
        def get_trg_shape():
            return trg.shape[1]
        batch_size = get_trg_shape()
        def max_len_output(max_len, batch_size, trg_vocab_size):
            return torch.zeros(max_len, batch_size, trg_vocab_size).to(device)

        max_len = trg.shape[0]

        trg_vocab_size = self.decoder.output_dim

        outputs = max_len_output(max_len, batch_size, trg_vocab_size)

        encoder_states, encoder_hidden = self.encoder(src)

        decoder_input = trg[0]

        t=1
        while t<(max_len ):
            decoder_output, decoder_hidden = self.decoder(decoder_input,encoder_states,encoder_hidden)
            def decoder_ouput(max_pr):
                a=trg[t] if random.random()<teacher_forcing_ratio else max_pr
                return a
            outputs[t] = decoder_output
            max_pr=decoder_output.argmax(1)
            decoder_input = decoder_ouput(max_pr)
            t+=1
        return outputs



def string_indices(trg, t_idx_to_char):
    """
    This function processes batches of indices into strings with the assistance of the supplied index-to-character mapping.

    Parameters:
        t_idx_to_char (Dict): A dictionary associating indices with characters.
        trg (Tensor): Tensor data containing encoder words, structured as batch_size x sequence_length.

    """

    sq=trg.shape[1]
    bs=trg.shape[0]
    strings = []

    i=0
    while i<(bs):
        chars = []
        for j in range(sq):
            def get_char(t_idx_to_char,trg,i,j):
                return t_idx_to_char[trg[i,j].item()]
            if trg[i,j].item() in t_idx_to_char:
                char = get_char(t_idx_to_char,trg,i,j)
                chars.append(char)
        string = ''.join(chars)

        strings.append(string)
        i+=1
    return strings

def Word_Accuracy1(model,t_idx_to_char,data_loader, criterion):
    '''
    This function computes the word-level accuracy following each epoch of training.

    Parameters:
    model: The trained model instance.
    t_idx_to_char: A mapping from target indices to characters.
    data_loader: DataLoader object for the validation or test dataset.
    criterion: The loss criterion employed during model training.
    '''
    model.eval()
    def set_zero():
        return 0
    epoch_loss = set_zero()
    num_total = set_zero()
    num_correct = set_zero()
    with torch.no_grad():
        for batch_idx, (src, trg, src_len, trg_len) in enumerate(data_loader):
            # Convert target indices to string for comparison
            string_trg=string_indices(trg,t_idx_to_char)
            # Move tensors to the device
            def set_permute(var):
                return var.permute(1, 0)
            src = set_permute(src)
            src = src.to(device)
            def output_reshape(output):
                return output[1:].reshape(-1, output.shape[2])
            trg = set_permute(trg)
            trg = trg.to(device)
            # Perform forward pass through the model
            output = model(src, trg, 0)
            # turn off teacher forcing
            output = output_reshape(output)
            trg = trg[1:].reshape(-1) # exclude the start-of-sequence token

            # Calculate the loss
            output = output.to(device)
            def get_bs(trg_len):
                return trg_len.shape[0]
            loss = criterion(output, trg)
            epoch_loss += loss.item()

            batch_size = get_bs(trg_len)


            seq_length = int(trg.numel() / batch_size)

            def get_predicted_indices(seq_length,predicted_indices):
                return predicted_indices.reshape(seq_length,-1)

            # Convert the output to predicted characters
            predicted_indices = torch.argmax(output, dim=1)
            predicted_indices = get_predicted_indices(seq_length,predicted_indices)
            predicted_indices = predicted_indices.permute(1, 0)
            # Convert predicted indices to strings
            string_pred=string_indices(predicted_indices,t_idx_to_char)

            for i in range(batch_size):
                num_total+=1
                def getlen_str():
                    return string_pred[i][:len(string_trg[i])] == string_trg[i]
                # Compare the predicted string with the target string
                if getlen_str():
                    num_correct+=1

    print("Total",num_total)
    print("Correct",num_correct)
    # Calculate word-level accuracy and average loss
    return ((num_correct) /num_total) * 100, (epoch_loss/(len(data_loader)))

def Word_Accuracy(model,t_idx_to_char,s_idx_to_char,data_loader, criterion):
    '''
    This function is used for the test data
    Parameters:
    model: Trained model object.
    t_idx_to_char: Index-to-character mapping for the target language.
    s_idx_to_char: Index-to-character mapping for the source language.
    data_loader: DataLoader for the validation or test dataset.
    criterion: Loss criterion utilized during model training.
    '''

    model.eval()
    def set_zero():
        return 0
    i_pred=[]
    i_trg=[]
    num_correct = set_zero()
    c_pred=[]
    c_src=[]
    num_total = set_zero()
    c_trg=[]
    epoch_loss = set_zero()
    i_src=[]

    with torch.no_grad():
        def get_s_indices(trg,t_idx_to_char):
            return string_indices(trg,t_idx_to_char)
        for batch_idx, (src, trg, src_len, trg_len) in enumerate(data_loader):
            # Convert target indices to string for comparison
            string_trg = get_s_indices(trg,t_idx_to_char)
            def set_permute(var):
                return var.permute(1, 0)
            string_src=string_indices(src,s_idx_to_char)
            # Move tensors to the device
            src = set_permute(src)
            src = src.to(device)
            trg = set_permute(trg)
            trg = trg.to(device)
            # Perform forward pass through the model
            def output_reshape(output):
                return output[1:].reshape(-1, output.shape[2])
            output = model(src, trg, 0)
            # turn off teacher forcing
            output = output_reshape(output)
            #print("op after ",output.shape) # exclude the start-of-sequence token

            trg = trg[1:].reshape(-1) # exclude the start-of-sequence token
            #print("trg after reshape",trg.shape)
            def get_crit(output,trg):
                return criterion(output, trg)
            # Calculate the loss
            output = output.to(device)
            def get_seq_len(trg,batch_size):
              return int(trg.numel() / batch_size)
            loss = get_crit(output,trg)
            epoch_loss += loss.item()
            batch_size = trg_len.shape[0]
            #print("bs", batch_size)
            seq_length = get_seq_len(trg,batch_size)

            def get_indice_reshape(predicted_indices,seq_length):
                return predicted_indices.reshape(seq_length,-1)
            # Convert the output to predicted characters
            predicted_indices = torch.argmax(output, dim=1)
            predicted_indices = get_indice_reshape(predicted_indices,seq_length)
            predicted_indices = predicted_indices.permute(1, 0)
            # Convert predicted indices to strings
            string_pred=string_indices(predicted_indices,t_idx_to_char)

            for i in range(batch_size):
                num_total+=1
                def get_condition_check(string_pred,string_trg):
                    return string_pred[i][:len(string_trg[i])] == string_trg[i]
                # Compare the predicted string with the target string
                def update_trg(c_trg,string_trg):
                    c_trg.append(string_trg[i])
                    return c_trg
                if get_condition_check(string_pred,string_trg):
                    c_trg=update_trg(c_trg,string_trg)
                    c_src.append(string_src[i])
                    def ret_one():
                        return 1
                    c_pred.append(string_pred[i][:len(string_trg[i])])
                    num_correct+=ret_one()
                else :
                    i_trg.append(string_trg[i])
                    def get_updation():
                        return string_pred[i][:len(string_trg[i])]
                    i_src.append(string_src[i])
                    i_pred.append(get_updation())



    def cal_avg_acc(num_correct ,num_total):
        return num_correct /num_total
    print("Total",num_total)
    print("Correct",num_correct)
    acc=cal_avg_acc(num_correct ,num_total)
    loss_e=(epoch_loss/(len(data_loader)))
    return acc * 100,loss_e ,c_trg,c_src,c_pred,i_trg,i_src,i_pred




def train(args):

    # This function trains the Seq2Seq model using the arguments passed to it.
    # The function begins by initializing Weights and Biases to log the training process.
    # It then extracts the hyperparameters from the provided arguments for configuring the model.
    # The training loop runs for the specified number of epochs.
    # Within the loop, it iterates through the batches in the training data, performing forward and backward passes to update the model parameters.
    # Training metrics such as loss and accuracy are logged and printed periodically.
    # Finally, the best model is saved, and the Weights and Biases run is completed.
    # The function returns when the training is finished.


    def get_celltype():
      return args.cell_type
    def get_tf():
        return args.teacher_forcing
    tf=get_tf()
    def get_dropout():
        return args.dropout
    def get_bidir():
        return False
    bidir = get_bidir()
    dp = get_dropout()
    def get_numlayers():
        return args.num_layers
    def get_batch_size():
        return args.batch_size
    bs = get_batch_size()
    epochs = 25
    def get_lr():
        return args.learning_rate
    lr = get_lr()
    hs=args.hidden_size
    opt= args.optim
    ct='gru'
    trg_pad_idx=0
    em=args.embedding_size
    nlayer=get_numlayers()
    INPUT_DIM = 29
    OUTPUT_DIM = 67

    name = "attention_"+"cell_type_"+str(get_celltype())+"_num_layers_"+str(get_numlayers())+"_dp_"+str(get_dropout())+"_bidir_"+str(get_bidir())+"_lr_"+str(get_lr())+"_bs_"+str(get_batch_size())
    wandb.run.name=name
  # Load the dataset
    train_loader,val_loader,test_loader,idx_to_char,s_idx_to_char=data_loading(bs)

  #print("data loaded ====================================================")
    def get_critirion():
        return nn.CrossEntropyLoss()
  # Instantiate the Encoder and Decoder models
    encoder = Encoder(INPUT_DIM,em,hs,nlayer,bidir,ct,dp).to(device)
    decoder = Decoder(OUTPUT_DIM,em,hs,nlayer,bidir,ct,dp).to(device)

  # Instantiate the Seq2Seq model with the Encoder and Decoder models
    model = Seq2Seq(encoder,decoder,ct,bidir).to(device)
  #print("model ini==============================================================")

  # Define the loss function and optimizer
    criterion = get_critirion()
    def get_optim_nadam():
        return optim.NAdam(model.parameters(),lr=lr)
    def get_optim_adam():
        return optim.Adam(model.parameters(),lr=lr)
    if opt == "nadam":
          optimizer= get_optim_nadam()
    elif opt == "adam":
          optimizer = get_optim_adam()

  # Train Network
    epoch=0
    while epoch < (epochs):
        def permutation(val):
          return val.permute(1, 0)
        model.train()
        epoch_loss = 0
        for batch_idx, (src, trg, src_len, trg_len) in enumerate(train_loader):
            src = permutation(src)  # swapping the dimensions of src tensor
            src = src.to(device)
            trg = permutation(trg)  # swapping the dimensions of trg tensor
            trg = trg.to(device)

            optimizer.zero_grad()
            def reshaping(output):
                return output[1:].reshape(-1, output.shape[2])
            output = model(src,trg,tf)

            output = reshaping(output)
            def get_loss(output, trg):
                return criterion(output, trg)
            trg = trg[1:].reshape(-1)

            loss = get_loss(output, trg)
            loss.backward()
            def get_item(loss):
                return loss.item()
            optimizer.step()
            epoch_loss += get_item(loss)

            if batch_idx % 1000 == 0:
                print(f"Epoch: {epoch}, Batch: {batch_idx} , Training..")

        # Calculate word-level accuracy after every epoch
        train_acc ,train_loss= Word_Accuracy1(model,idx_to_char, train_loader,criterion)
        def get_test_acc():
          return Word_Accuracy1(model,idx_to_char, test_loader, criterion)
        val_acc,val_loss = Word_Accuracy1(model,idx_to_char, val_loader, criterion)
        test_acc,test_loss = get_test_acc()

        print(f"Epoch: {epoch}, Loss: {epoch_loss / len(train_loader)}, Train Acc: {train_acc}, Val Acc: {val_acc}")
    # Log the metrics to WandB
        wandb.log({'epoch': epochs,'train_acc':train_acc, 'train_loss': loss.item(),'val_acc': val_acc,'val_loss': val_loss, 'test_acc': test_acc,'test_loss': test_loss})
    # Save the best model
        epoch+=1
    wandb.run.save()
    wandb.run.finish()
    return





wandb_key = input("Enter your WandB API key: ")

wandb.login(key=wandb_key)
# Create an argument parser object
# Add arguments for model hyperparameters


parser = argparse.ArgumentParser(description="Train a model with specified parameters or perform hyperparameter tuning using wandb sweeps")
parser = argparse.ArgumentParser(description="Stores all the hyperparameters for the model.")

parser = argparse.ArgumentParser()
# Cell type: Choices include 'rnn', 'gru', and 'lstm'
# Batch size used for training the neural network
# Optimization algorithm: Choices include 'adam' and 'nadam'
# Learning rate used to optimize model parameters
# Size of embedding
# Hidden size: Choices include 64, 128, 256, and 512
# Dropout rate: Choices include 0, 0.2, and 0.3
# Number of layers in the network
# Bidirectional flag: Choices include True and False
# Teacher forcing ratio: Choices include 0, 0.2, 0.3, 0.5, and 0.7

parser.add_argument('-wp' , '--wandb_project', help='Project name used to track experiments in Weights & Biases dashboard' , type=str, default='Assignment_3_DL_test')
parser.add_argument('-we', '--wandb_entity' , help='Wandb Entity used to track experiments in the Weights & Biases dashboard.' , type=str, default='cs23m030')
parser.add_argument('-ct', '--cell_type', help="Choices:['rnn','gru','lstm']", type=str, default='lstm')
parser.add_argument('-b', '--batch_size', help="Batch size used to train neural network.", type=int, default=128)
parser.add_argument('-o', '--optim', help = 'choices: [ "adam", "nadam"]', type=str, default = 'adam')
parser.add_argument('-lr', '--learning_rate', help = 'Learning rate used to optimize model parameters', type=float, default=0.001)
parser.add_argument('-em', '--embedding_size', help='size of embedding', type=int, default=512)
parser.add_argument('-hs', '--hidden_size', help='choices:[64,128,256,512]',type=int, default=512)
parser.add_argument('-dp', '--dropout', help='choices:[0,0.2,0.3]',type=float, default=0.2)
parser.add_argument('-nl', '--num_layers', help='Number of layers in network ',type=int, default=3)
parser.add_argument('-bidir', '--bidirectional', help='Choices:["True","False"]',type=bool, default=False)
parser.add_argument('-tf', '--teacher_forcing', help='choices:[0,0.2,0.3,0.5,0.7]',type=float, default=0.7)

args = parser.parse_args()


wandb.init(project=args.wandb_project)
train(args)