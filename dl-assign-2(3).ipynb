{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"gpuClass":"standard","kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"widgets":{"application/vnd.jupyter.widget-state+json":{"954db47fab7b49079e0d36719a9d86f6":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_9ae01b86c4294b31a5cfe704e705bd4d","IPY_MODEL_41132bb11e864ad495e08bff366cd140"],"layout":"IPY_MODEL_045af002be2c4722ae72f1ed49180b73"}},"9ae01b86c4294b31a5cfe704e705bd4d":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cbd7c85e1bfd41778c675d449da5558d","placeholder":"​","style":"IPY_MODEL_56eb7909fa6a45579c8d9d86a7279576","value":"0.011 MB of 0.011 MB uploaded\r"}},"41132bb11e864ad495e08bff366cd140":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_314420b6029747f5a40aa20a6da0c693","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_27e7af3119a94299ae4121fe6f1400e8","value":1}},"045af002be2c4722ae72f1ed49180b73":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cbd7c85e1bfd41778c675d449da5558d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"56eb7909fa6a45579c8d9d86a7279576":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"314420b6029747f5a40aa20a6da0c693":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"27e7af3119a94299ae4121fe6f1400e8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"289ba190978547cead0b800c4b025692":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_8d2b8415968c41b48eeb2b38a58b4b1c","IPY_MODEL_f1bcf83005664a218fdb4de4dd549758"],"layout":"IPY_MODEL_c2f09eebc9d5434cb8fff64f9588474b"}},"8d2b8415968c41b48eeb2b38a58b4b1c":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_050ebaf3095e491cb1fa2f2c0d6f5ff7","placeholder":"​","style":"IPY_MODEL_b9e32927f9974dd8b013ca3f96b77190","value":"0.011 MB of 0.011 MB uploaded\r"}},"f1bcf83005664a218fdb4de4dd549758":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_cac9be591c9d454f81e29789c0eea0d6","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c927fdbcf04f4d1ca5581c88fd2b6b18","value":1}},"c2f09eebc9d5434cb8fff64f9588474b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"050ebaf3095e491cb1fa2f2c0d6f5ff7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9e32927f9974dd8b013ca3f96b77190":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cac9be591c9d454f81e29789c0eea0d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c927fdbcf04f4d1ca5581c88fd2b6b18":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"09c2ae72236c4bf09965f20216c8bc7f":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_ba12c27ce4304f13b8368b51956817ef","IPY_MODEL_2540451c1db5483a8ff0ae243bf675b5"],"layout":"IPY_MODEL_81cb0f2164be47c08fedb203d2bd89dd"}},"ba12c27ce4304f13b8368b51956817ef":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f50c40e7e02d434b9d47576e077e772c","placeholder":"​","style":"IPY_MODEL_d75c43f6013e40a09d12becbb08d4535","value":"0.011 MB of 0.011 MB uploaded\r"}},"2540451c1db5483a8ff0ae243bf675b5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_4f9fa4b4d7cc4334a8012c612a217ee5","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3c1cc9ec32114c8d948994b248aa958b","value":1}},"81cb0f2164be47c08fedb203d2bd89dd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f50c40e7e02d434b9d47576e077e772c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d75c43f6013e40a09d12becbb08d4535":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4f9fa4b4d7cc4334a8012c612a217ee5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3c1cc9ec32114c8d948994b248aa958b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a8e81eeb77ed468fbafed1ae3c684d4e":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_09c4dbfa12214784a95690ae3de988a1","IPY_MODEL_befa9aa38c2c48929e0b9fffb7a6e01c"],"layout":"IPY_MODEL_5623ab46621d4b14a0e24f43593fe16a"}},"09c4dbfa12214784a95690ae3de988a1":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3f7a93b196cd48fa9d5e10e82cda9f51","placeholder":"​","style":"IPY_MODEL_6f01b7b37f9943c3970f1c06858b082f","value":"0.011 MB of 0.011 MB uploaded\r"}},"befa9aa38c2c48929e0b9fffb7a6e01c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_578051e20bb54143955be18f6850dbde","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dcd54565bb68461b940ee4b76098b02c","value":1}},"5623ab46621d4b14a0e24f43593fe16a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f7a93b196cd48fa9d5e10e82cda9f51":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6f01b7b37f9943c3970f1c06858b082f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"578051e20bb54143955be18f6850dbde":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dcd54565bb68461b940ee4b76098b02c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9557cfd077b6423f83d7dcfff85d4eb2":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_ad720fe0738943c98a3954bc21934fff","IPY_MODEL_e3ba0044a36d419791c12e36968492ce"],"layout":"IPY_MODEL_1c4db8afdffa4dd9b9b1a57a88e07533"}},"ad720fe0738943c98a3954bc21934fff":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6c12805654544c2cb72edfd7a3009726","placeholder":"​","style":"IPY_MODEL_78c45c983b2b489f9abe47f33ee20e1c","value":"0.011 MB of 0.011 MB uploaded\r"}},"e3ba0044a36d419791c12e36968492ce":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_facadb507b25419e96115a48020f4486","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c55a9a455b1944c082ee12c642760cd1","value":1}},"1c4db8afdffa4dd9b9b1a57a88e07533":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c12805654544c2cb72edfd7a3009726":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"78c45c983b2b489f9abe47f33ee20e1c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"facadb507b25419e96115a48020f4486":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c55a9a455b1944c082ee12c642760cd1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"707f836fccae4aab8d0446d9639b5dc6":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_f27288e70fc24bf79052c7ca76419ea9","IPY_MODEL_9375c38454a341bb893249cf5e51c135"],"layout":"IPY_MODEL_96d157f0d315420e82264b28c33aa235"}},"f27288e70fc24bf79052c7ca76419ea9":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_64e450881dd447e6a203f7eef3415d4d","placeholder":"​","style":"IPY_MODEL_affbe0eb270c466296dc6c0a257c39c1","value":"0.011 MB of 0.011 MB uploaded\r"}},"9375c38454a341bb893249cf5e51c135":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_398dcd51a6ea4f0faf22c55c6c4264f4","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f936a9f2311444b4aed714779fa57db6","value":1}},"96d157f0d315420e82264b28c33aa235":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"64e450881dd447e6a203f7eef3415d4d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"affbe0eb270c466296dc6c0a257c39c1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"398dcd51a6ea4f0faf22c55c6c4264f4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f936a9f2311444b4aed714779fa57db6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}}},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8322101,"sourceType":"datasetVersion","datasetId":4943540},{"sourceId":8339465,"sourceType":"datasetVersion","datasetId":4952986},{"sourceId":8348482,"sourceType":"datasetVersion","datasetId":4959739}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport torch\n# Print the name of the CUDA device, if available\nprint(torch.device('cuda:0'))\n# Print the version of the torch library\nprint(torch.__version__)\n\n# Create a variable to store the device to be used\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Print the device that will be used\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"id":"ys8_6uZsFwpc","outputId":"66513628-a743-449d-ef6e-34289b5a5f29","execution":{"iopub.status.busy":"2024-05-08T21:31:59.050551Z","iopub.execute_input":"2024-05-08T21:31:59.051465Z","iopub.status.idle":"2024-05-08T21:32:02.210625Z","shell.execute_reply.started":"2024-05-08T21:31:59.051432Z","shell.execute_reply":"2024-05-08T21:32:02.209624Z"},"editable":false,"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"cuda:0\n2.1.2\ncuda\n","output_type":"stream"}]},{"cell_type":"code","source":"# from google.colab import drive\n# drive.mount('/content/drive')","metadata":{"id":"f-aHQ6A1KBst","outputId":"c41ce815-838b-4777-8046-4c7f6243d83a","execution":{"iopub.status.busy":"2024-05-08T21:32:02.212320Z","iopub.execute_input":"2024-05-08T21:32:02.212699Z","iopub.status.idle":"2024-05-08T21:32:02.216695Z","shell.execute_reply.started":"2024-05-08T21:32:02.212673Z","shell.execute_reply":"2024-05-08T21:32:02.215723Z"},"editable":false,"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# imports\nimport torch.nn as nn\nimport torch.optim as optim\n\nimport pandas as pd\nimport math\nimport torch\nimport torchvision\nimport torch.nn.functional as F  # Parameterless functions, like (some) activation functions\nimport torchvision.datasets as datasets  # Standard datasets\nimport torchvision.transforms as transforms  # Transformations we can perform on our dataset for augmentation\nfrom torch import optim  # For optimizers like SGD, Adam, etc.\nfrom torch import nn  # All neural network modules\nfrom torch.utils.data import (\n    DataLoader, random_split\n)  # Gives easier dataset managment by creating mini batches etc.\nfrom tqdm import tqdm  # For nice progress bar!\n\nfrom torchvision.datasets import ImageFolder\nimport os\nimport random\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pathlib\n","metadata":{"id":"KPLdU191FxaW","execution":{"iopub.status.busy":"2024-05-08T21:32:02.218034Z","iopub.execute_input":"2024-05-08T21:32:02.218436Z","iopub.status.idle":"2024-05-08T21:32:05.213987Z","shell.execute_reply.started":"2024-05-08T21:32:02.218404Z","shell.execute_reply":"2024-05-08T21:32:05.213037Z"},"editable":false,"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"\n\"\"\"\n  This function sets the random seed for all major Python libraries.\n\n  Args:\n    seed (int): The random seed to use.\n\n  \"\"\"\ndef seed_everything(seed=1):\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\nseed_everything()","metadata":{"id":"sbHpQ-qgFzIa","execution":{"iopub.status.busy":"2024-05-08T21:32:05.216196Z","iopub.execute_input":"2024-05-08T21:32:05.216799Z","iopub.status.idle":"2024-05-08T21:32:05.226281Z","shell.execute_reply.started":"2024-05-08T21:32:05.216772Z","shell.execute_reply":"2024-05-08T21:32:05.225263Z"},"editable":false,"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Data Preprocessing\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport pandas as pd\n'''\nClass Vovabulary is used to create vocabulary from the training dataset.\n'''\nclass Vocabulary:\n    \"\"\"\n    Args:\n      file_path (string): The path to the CSV file containing the training data.\n      src_lang (string): The name of the source language.\n      trg_lang (string): The name of the target language.\n\n    Raises:\n      ValueError: If the file_path does not exist.\n\n    \"\"\"\n    def __init__(self, file_path, src_lang, trg_lang):\n        # Read the CSV file into a Pandas DataFrame.\n        self.translations = pd.read_csv(file_path, header=None, names=[src_lang, trg_lang])\n        # It will drop any rows with missing values\n        self.translations.dropna()\n        self.src_lang = src_lang\n        self.trg_lang = trg_lang\n        # Create a dictionary that maps each character in the source language to an integer index.\n        self.trg_vocab = {char: i+3 for i, char in enumerate(sorted(list(set(''.join(self.translations[trg_lang].tolist())))))}\n        # Create a dictionary that maps each character in the target language to an integer index.\n        self.src_vocab = {char: i+3 for i, char in enumerate(sorted(list(set(''.join(self.translations[src_lang].tolist())))))}\n\n        # Add special tokens to the vocabularies.\n        self.trg_vocab['<'] = 0\n        self.src_vocab['<'] = 0\n\n        self.trg_vocab['<unk>'] = 2\n        self.src_vocab['<pad>'] = 1\n        self.trg_vocab['<pad>'] = 1\n\n        self.src_vocab['<unk>'] = 2\n\n        # Extract the unique characters in the source and target languages\n        src_chars = sorted(set(''.join(self.translations[src_lang])))\n        trg_chars = sorted(set(''.join(self.translations[trg_lang])))\n\n        # Assign an index to each character in the source and target languages\n        self.t_char_to_idx = {char: idx+3 for idx, char in enumerate(trg_chars)}\n        self.t_char_to_idx['<unk>']=2\n        self.t_idx_to_char = {idx: char for char, idx in self.t_char_to_idx.items()}\n\n        self.s_char_to_idx = {char: idx+3 for idx, char in enumerate(src_chars)}\n        self.s_char_to_idx['<unk>']=2\n        self.s_idx_to_char = {idx: char for char, idx in self.s_char_to_idx.items()}\n\n\n\n\n    def get(self):\n         # This function returns the source and target vocabularies, as well as the dictionaries that map characters to integer indexes and vice versa.\n        return self.src_vocab,self.trg_vocab,self.t_char_to_idx,self.t_idx_to_char,self.s_char_to_idx,self.s_idx_to_char\n\n\n\nclass TransliterationDataset(Dataset):\n    \"\"\"\n    Args:\n      file_path (string): The path to the CSV file containing the training data.\n      src_lang (string): The name of the source language.\n      trg_lang (string): The name of the target language.\n      src_vocab (Vocabulary): The vocabulary for the source language.\n      trg_vocab (Vocabulary): The vocabulary for the target language.\n\n    Raises:\n      ValueError: If the file_path does not exist.\n\n    \"\"\"\n    def __init__(self, file_path, src_lang, trg_lang,src_vocab,trg_vocab,t_char_to_idx):\n        self.translations = pd.read_csv(file_path, header=None, names=[src_lang, trg_lang])\n        self.translations.dropna()\n\n        self.src_lang = src_lang\n        self.t_char_to_idx = t_char_to_idx\n        self.trg_lang = trg_lang\n        self.src_vocab = src_vocab\n        self.trg_vocab = trg_vocab\n        self.max_src_len = max([len(word) for word in self.translations[src_lang].tolist()])+1\n        #print(\"max src len\",self.max_src_len)\n        self.max_trg_len = max([len(word) for word in self.translations[trg_lang].tolist()])+1\n        #print(\"max trg len\",self.max_trg_len)\n    def __len__(self):\n        return len(self.translations)\n\n    def target_to_one_hot(self, target_word, char_to_idx):\n        num_trg_chars = len(char_to_idx)\n        max_target_len = self.max_trg_len\n        # Create a tensor of zeros for the one-hot encoding\n        one_hot = torch.zeros((max_target_len, num_trg_chars))\n        # Encode each character in the target word as a one-hot vector\n        for i, char in enumerate(target_word):\n            #print(i,char)\n            char_idx = char_to_idx[char if char in  char_to_idx else '<unk>']\n            #print(char_idx)\n            one_hot[i][char_idx] = 1\n        return one_hot\n\n    def __getitem__(self, idx):\n        src_word = self.translations.iloc[idx][self.src_lang]\n        trg_word = self.translations.iloc[idx][self.trg_lang]\n        #print(src_word)\n        # Initialize the start-of-word token\n        sow=0\n\n        # Convert source and target words to lists of vocabulary indices\n        src = [self.src_vocab.get(char, self.src_vocab['<unk>']) for char in src_word]\n        trg = [self.trg_vocab.get(char, self.src_vocab['<unk>']) for char in trg_word]\n        # Insert the start-of-word token at the beginning\n        src.insert(0, sow)\n        trg.insert(0, sow)\n\n        src_len = len(src)\n        trg_len = len(trg)\n\n        # Pad the source and target sequences with the <pad> token\n        src_pad = [self.src_vocab['<pad>']] * (self.max_src_len - src_len)\n        trg_pad = [self.trg_vocab['<pad>']] * (self.max_trg_len - trg_len)\n\n        # Extend the source and target sequences with padding\n        src.extend(src_pad)\n        trg.extend(trg_pad)\n\n        # Convert source and target sequences to tensors\n        src = torch.LongTensor(src)\n        trg = torch.LongTensor(trg)\n        #trg_one_hot = self.target_to_one_hot(trg_word, self.trg_vocab)\n        #src_one_hot = self.target_to_one_hot(src_word, self.src_vocab)\n\n        # This will return encoded source word ,target word and their length\n        return src, trg, src_len, trg_len\n","metadata":{"id":"WYH8LazuF1kK","execution":{"iopub.status.busy":"2024-05-08T21:32:05.227709Z","iopub.execute_input":"2024-05-08T21:32:05.228082Z","iopub.status.idle":"2024-05-08T21:32:05.251565Z","shell.execute_reply.started":"2024-05-08T21:32:05.228052Z","shell.execute_reply":"2024-05-08T21:32:05.250714Z"},"editable":false,"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def load_data(bs):\n    '''\n    This function loads data into batches provided the batch size as an argument.\n    '''\n    # Define the paths for the train, validation, and test CSV files\n    train_path  =\"/kaggle/input/akshantar/aksharantar_sampled/hin/hin_train.csv\"\n    val_path  =\"/kaggle/input/akshantar/aksharantar_sampled/hin/hin_valid.csv\"\n    test_path  =\"/kaggle/input/akshantar/aksharantar_sampled/hin/hin_test.csv\"\n\n    # Create a vocabulary object and retrieve the source and target vocabularies,\n    # character-to-index and index-to-character mappings\n    vocab = Vocabulary(train_path, 'src', 'trg')\n    src_vocab,trg_vocab,t_char_to_idx,t_idx_to_char,s_char_to_idx,s_idx_to_char=vocab.get()\n    #print(len(src_vocab))\n    #print(len(trg_vocab))\n    #print(\"char to idc outside\",char_to_idx)\n\n\n    # Create train, validation, and test datasets using TransliterationDataset\n    # with the appropriate source and target vocabularies and mappings\n    train_dataset = TransliterationDataset(train_path, 'src', 'trg',src_vocab,trg_vocab,t_char_to_idx)\n    val_dataset = TransliterationDataset(val_path, 'src', 'trg',src_vocab,trg_vocab,t_char_to_idx)\n    test_dataset = TransliterationDataset(test_path, 'src', 'trg',src_vocab,trg_vocab,t_char_to_idx)\n\n    # Create train, validation, and test data loaders\n    train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=bs, shuffle=False)\n    test_loader = DataLoader(test_dataset, batch_size=bs, shuffle=False)\n\n    return train_loader,test_loader,val_loader,t_idx_to_char,s_idx_to_char\n\n\n    #Training and check accuracy function\n\n\n#train_loader,test_loader,val_loader,idx_to_char=load_data(32)\n#print(idx_to_char)","metadata":{"id":"W50h47NjF3tF","execution":{"iopub.status.busy":"2024-05-08T21:32:05.252664Z","iopub.execute_input":"2024-05-08T21:32:05.253112Z","iopub.status.idle":"2024-05-08T21:32:05.266952Z","shell.execute_reply.started":"2024-05-08T21:32:05.253087Z","shell.execute_reply":"2024-05-08T21:32:05.266095Z"},"editable":false,"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Model\n\nclass Encoder(nn.Module):\n    def __init__(self, input_dim, embedded_size,hidden_dim, num_layers,bidirectional, cell_type,dp):\n        super(Encoder, self).__init__()\n\n        # Store the input dimensions, embedding size, hidden size, number of layers,\n        # bidirectional flag, cell type, and dropout probability\n        self.input_dim = input_dim\n        self.embedded_size=embedded_size\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n        self.cell_type = cell_type\n        self.bidirectional=bidirectional\n        self.dropout = nn.Dropout(dp)\n\n        # Determine the directionality of the encoder (1 for unidirectional, 2 for bidirectional)\n        if bidirectional:\n            self.dir=2\n        else:\n            self.dir=1\n        # Create an embedding layer\n        self.embedding = nn.Embedding(input_dim,embedded_size)\n\n        # Create the recurrent layer based on the specified cell type\n        if cell_type == 'rnn':\n              self.rnn = nn.RNN(embedded_size, hidden_dim, num_layers, dropout=dp,bidirectional=bidirectional)\n        elif cell_type == 'lstm':\n              self.rnn = nn.LSTM(embedded_size, hidden_dim, num_layers, dropout=dp,bidirectional=bidirectional)\n        elif cell_type == 'gru':\n              self.rnn = nn.GRU(embedded_size, hidden_dim, num_layers, dropout=dp,bidirectional=bidirectional)\n        else:\n            raise ValueError(\"Invalid cell type. Choose 'rnn', 'lstm', or 'gru'.\")\n\n    def forward(self, src):\n        # Apply dropout to the embedded input\n        embedded = self.dropout(self.embedding(src))\n        # If the cell type is LSTM, return both the output and the hidden and cell states in a single tuple\n        if self.cell_type == 'lstm':\n            output, (hidden, cell) = self.rnn(embedded)\n            return output, (hidden, cell)\n\n        else:\n            # For other cell types (RNN, GRU), return the output and the hidden state\n            output, hidden = self.rnn(embedded)\n            return output,hidden\n\n\n\n\nclass Decoder(nn.Module):\n    def __init__(self, output_dim,embedded_size, hidden_dim, num_layers,bidirectional,cell_type,dp):\n        super(Decoder, self).__init__()\n        # Store the input dimensions, embedding size, hidden size, number of layers,\n        # bidirectional flag, cell type, and dropout probabilit\n        self.output_dim = output_dim\n        self.embedded_size=embedded_size\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n        self.cell_type = cell_type\n        self.bidirectional=bidirectional\n        self.dropout = nn.Dropout(dp)\n        if bidirectional:\n            self.dir=2\n        else:\n            self.dir=1\n\n        # Create an embedding layer\n        self.embedding = nn.Embedding(output_dim,embedded_size)\n        # Create the recurrent layer based on the specified cell type\n        if cell_type == 'rnn':\n            self.rnn = nn.RNN(embedded_size, hidden_dim, num_layers,dropout=dp)\n        elif cell_type == 'lstm':\n            self.rnn = nn.LSTM(embedded_size, hidden_dim, num_layers,dropout=dp)\n        elif cell_type == 'gru':\n            self.rnn = nn.GRU(embedded_size, hidden_dim, num_layers,dropout=dp)\n        else:\n            raise ValueError(\"Invalid cell type. Choose 'rnn', 'lstm', or 'gru'.\")\n\n        # Create the output fully connected layer\n        self.fc_out = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, input, hidden):\n        embedded = self.dropout(self.embedding(input))\n        # Pass the embedded input and hidden state through the decoder RNN\n        output, hidden = self.rnn(embedded, hidden)\n        # Pass the decoder output through the fully connected layer\n        output = self.fc_out(output)\n        # Apply log softmax activation to the output\n        output = F.log_softmax(output, dim=1)\n\n        return output, hidden\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder,cell_type,bidirectional):\n        super(Seq2Seq, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.cell_type=cell_type\n        self.bidirectional=bidirectional\n\n    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n        batch_size = trg.shape[1]\n        #print(batch_size)\n        max_len = trg.shape[0]\n        #print(max_len)\n        trg_vocab_size = self.decoder.output_dim\n\n        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(device)\n\n        encoder_output, encoder_hidden = self.encoder(src)\n        #print(\"encoder hidden shape\",encoder_hidden.shape)\n        # Concatenate the last hidden state of the encoder from both directions\n        if self.bidirectional:\n            if self.cell_type=='lstm':\n                hidden_concat = torch.add(encoder_hidden[0][0:self.encoder.num_layers,:,:], encoder_hidden[1][0:self.encoder.num_layers,:,:])/2\n                cell_concat = torch.add(encoder_hidden[0][self.encoder.num_layers:,:,:], encoder_hidden[1][self.encoder.num_layers:,:,:])/2\n                hidden_concat = (hidden_concat, cell_concat)\n\n            else:\n                hidden_concat = torch.add(encoder_hidden[0:self.encoder.num_layers,:,:], encoder_hidden[self.encoder.num_layers:,:,:])/2\n        else:\n            hidden_concat= encoder_hidden\n\n        decoder_hidden = hidden_concat\n        # Initialize decoder input with the start token\n        decoder_input = (trg[0,:]).unsqueeze(0)\n        #print(\"decoder input shape\",decoder_input.shape)\n\n        for t in range(1,trg.shape[0] ):\n\n            # Pass the decoder input and hidden state through the decoder\n            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n\n            # Store the decoder output in the outputs tensor\n            outputs[t] = decoder_output\n            # Determine the next decoder input using teacher forcing or predicted output\n            max_pr, idx=torch.max(decoder_output,dim=2)\n            #print(\"trg shape\",trg.shape)\n            idx=idx.view(trg.shape[1])\n            teacher_force = torch.rand(1) < teacher_forcing_ratio\n            if teacher_force:\n                decoder_input= trg[t,:].unsqueeze(0)\n            else:\n                decoder_input= idx.unsqueeze(0)\n\n         # Pass the last decoder input and hidden state through the decoder\n        decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n\n        # Store the final decoder output in the outputs tensor\n        #outputs[-1] = decoder_output\n        return outputs\n","metadata":{"id":"VpRLyDoRGCi6","execution":{"iopub.status.busy":"2024-05-08T21:32:05.268194Z","iopub.execute_input":"2024-05-08T21:32:05.268520Z","iopub.status.idle":"2024-05-08T21:32:05.295520Z","shell.execute_reply.started":"2024-05-08T21:32:05.268497Z","shell.execute_reply":"2024-05-08T21:32:05.294712Z"},"editable":false,"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def indices_to_string(trg, t_idx_to_char):\n    \"\"\"Converts a batch of indices to strings using the given index-to-char mapping\n    Args:\n    trg(Tensor):encoder words of size batch_size x sequence length\n    t_idx_to_char(Dict.): index to char mapping\n\n    \"\"\"\n    strings = []\n    bs=trg.shape[0]\n    sq=trg.shape[1]\n    for i in range(bs):\n      chars = []\n      #print(i)\n      # Convert the sequence of indices to a sequence of characters using the index-to-char mapping\n      for j in range(sq):\n        #print(j)\n        #print(trg[i,j].item())\n        if trg[i,j].item() in t_idx_to_char:\n          #print(trg[i,j])\n          char = t_idx_to_char[trg[i,j].item()]\n          chars.append(char)\n            #print(chars)\n      # Join the characters into a string\n      string = ''.join(chars)\n      #print(string)\n        # Append the string to the list of strings\n      strings.append(string)\n    return strings\n","metadata":{"id":"S4cPKpxWGALj","execution":{"iopub.status.busy":"2024-05-08T21:32:05.296698Z","iopub.execute_input":"2024-05-08T21:32:05.297138Z","iopub.status.idle":"2024-05-08T21:32:05.309330Z","shell.execute_reply.started":"2024-05-08T21:32:05.297106Z","shell.execute_reply":"2024-05-08T21:32:05.308451Z"},"editable":false,"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"\ndef calculate_word_level_accuracy(model,t_idx_to_char,data_loader, criterion):\n    '''\n    This function will calculate word level accuracy after each epoch.\n    Args:\n        model: The trained model\n        t_idx_to_char: Mapping from target indices to characters\n        data_loader: Data loader for the validation/test dataset\n        criterion: Loss criterion used for training the model\n\n\n    '''\n    model.eval()\n    num_correct = 0\n    num_total = 0\n    epoch_loss = 0\n\n    with torch.no_grad():\n        for batch_idx, (src, trg, src_len, trg_len) in enumerate(data_loader):\n            # Convert target indices to string for comparison\n            string_trg=indices_to_string(trg,t_idx_to_char)\n            # Move tensors to the device\n            src = src.permute(1, 0)\n            trg = trg.permute(1, 0)\n            src = src.to(device)\n            trg = trg.to(device)\n            # Perform forward pass through the model\n            output = model(src, trg, 0)\n            # turn off teacher forcing\n            output = output[1:].reshape(-1, output.shape[2])\n            #print(\"op after \",output.shape) # exclude the start-of-sequence token\n\n            trg = trg[1:].reshape(-1) # exclude the start-of-sequence token\n            #print(\"trg after reshape\",trg.shape)\n\n            # Calculate the loss\n            output = output.to(device)\n            loss = criterion(output, trg)\n            epoch_loss += loss.item()\n\n            batch_size = trg_len.shape[0]\n            #print(\"bs\", batch_size)\n            seq_length = int(trg.numel() / batch_size)\n\n\n            # Convert the output to predicted characters\n            predicted_indices = torch.argmax(output, dim=1)\n            predicted_indices = predicted_indices.reshape(seq_length,-1)\n            predicted_indices = predicted_indices.permute(1, 0)\n            # Convert predicted indices to strings\n            string_pred=indices_to_string(predicted_indices,t_idx_to_char)\n            #print(string_pred)\n            #print(string_trg)\n\n            for i in range(batch_size):\n                num_total+=1\n                # Compare the predicted string with the target string\n                if string_pred[i][:len(string_trg[i])] == string_trg[i]:\n                    num_correct+=1\n\n    print(\"Total\",num_total)\n    print(\"Correct\",num_correct)\n    # Calculate word-level accuracy and average loss\n    return (num_correct /num_total) * 100, (epoch_loss/(len(data_loader)))\n","metadata":{"id":"24EGz_7-GK74","execution":{"iopub.status.busy":"2024-05-08T21:32:05.310601Z","iopub.execute_input":"2024-05-08T21:32:05.311106Z","iopub.status.idle":"2024-05-08T21:32:05.322445Z","shell.execute_reply.started":"2024-05-08T21:32:05.311076Z","shell.execute_reply":"2024-05-08T21:32:05.321543Z"},"editable":false,"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"\ndef calculate_word_level_accuracy1(model,t_idx_to_char,s_idx_to_char,data_loader, criterion):\n    '''\n    This function is just extension of above function and\n    will calculate word level accuracy as well as store the correct and\n    incorrect words into list .\n    We'll call this function after training only once for test data.\n    Args:\n        model: The trained model\n        t_idx_to_char: Mapping from target indices to characters\n        s_idx_to_char: Mapping from source indices to characters\n        data_loader: Data loader for the validation/test dataset\n        criterion: Loss criterion used for training the model\n\n\n    '''\n    model.eval()\n    num_correct = 0\n    num_total = 0\n    epoch_loss = 0\n    c_trg=[]\n    c_src=[]\n    c_pred=[]\n\n    i_trg=[]\n    i_src=[]\n    i_pred=[]\n\n\n    with torch.no_grad():\n        for batch_idx, (src, trg, src_len, trg_len) in enumerate(data_loader):\n            # Convert target indices to string for comparison\n            string_trg=indices_to_string(trg,t_idx_to_char)\n            string_src=indices_to_string(src,s_idx_to_char)\n\n            # Move tensors to the device\n            src = src.permute(1, 0)\n            trg = trg.permute(1, 0)\n            src = src.to(device)\n            trg = trg.to(device)\n            # Perform forward pass through the model\n            output = model(src, trg, 0)\n            # turn off teacher forcing\n            output = output[1:].reshape(-1, output.shape[2])\n            #print(\"op after \",output.shape) # exclude the start-of-sequence token\n\n            trg = trg[1:].reshape(-1) # exclude the start-of-sequence token\n            #print(\"trg after reshape\",trg.shape)\n\n            # Calculate the loss\n            output = output.to(device)\n            loss = criterion(output, trg)\n            epoch_loss += loss.item()\n\n            batch_size = trg_len.shape[0]\n            #print(\"bs\", batch_size)\n            seq_length = int(trg.numel() / batch_size)\n\n\n            # Convert the output to predicted characters\n            predicted_indices = torch.argmax(output, dim=1)\n            predicted_indices = predicted_indices.reshape(seq_length,-1)\n            predicted_indices = predicted_indices.permute(1, 0)\n            # Convert predicted indices to strings\n            string_pred=indices_to_string(predicted_indices,t_idx_to_char)\n            #print(string_pred)\n            #print(string_trg)\n\n            for i in range(batch_size):\n                num_total+=1\n                # Compare the predicted string with the target string\n                if string_pred[i][:len(string_trg[i])] == string_trg[i]:\n                  c_trg.append(string_trg[i])\n                  c_src.append(string_src[i])\n                  c_pred.append(string_pred[i][:len(string_trg[i])])\n                  num_correct+=1\n                else :\n                  i_trg.append(string_trg[i])\n                  i_src.append(string_src[i])\n                  i_pred.append(string_pred[i][:len(string_trg[i])])\n\n\n\n    print(\"Total\",num_total)\n    print(\"Correct\",num_correct)\n    # Calculate word-level accuracy and average loss\n    return (num_correct /num_total) * 100, (epoch_loss/(len(data_loader))),c_trg,c_src,c_pred,i_trg,i_src,i_pred\n\n\n","metadata":{"id":"7TTpm6ZXhyiw","execution":{"iopub.status.busy":"2024-05-08T21:32:05.326229Z","iopub.execute_input":"2024-05-08T21:32:05.326493Z","iopub.status.idle":"2024-05-08T21:32:05.341063Z","shell.execute_reply.started":"2024-05-08T21:32:05.326471Z","shell.execute_reply":"2024-05-08T21:32:05.340319Z"},"editable":false,"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"\n# # Define hyperparameters\n# INPUT_DIM = 29\n# OUTPUT_DIM = 67\n# embedding_size=512\n# HIDDEN_DIM = 512\n# NUM_LAYERS = 3\n# CELL_TYPE = 'gru'\n# BATCH_SIZE = 128\n# LEARNING_RATE = 0.0002\n# TEACHER_FORCING_RATIO = 0.7\n# EPOCHS = 25\n\n# dropout=0.1\n# bidirectional=True\n# opt='adam'\n\n\n# # Load data and create data loaders\n# train_loader,test_loader,val_loader,t_idx_to_char,s_idx_to_char=load_data(BATCH_SIZE)\n# #print(len(test_loader))\n# #print(len(train_loader))\n# #print(len(val_loader))\n# # Instantiate the Encoder and Decoder models\n# encoder = Encoder(INPUT_DIM,embedding_size,HIDDEN_DIM, NUM_LAYERS,bidirectional, CELL_TYPE,dropout).to(device)\n# decoder = Decoder(OUTPUT_DIM,embedding_size,HIDDEN_DIM, NUM_LAYERS,bidirectional,CELL_TYPE,dropout).to(device)\n\n# # Instantiate the Seq2Seq model with the Encoder and Decoder models\n# model = Seq2Seq(encoder, decoder,CELL_TYPE,bidirectional).to(device)\n\n# # Define the loss function and optimizer\n# criterion = nn.CrossEntropyLoss()\n# optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n# #optimizer=optimizer(model,opt,LEARNING_RATE)\n\n\n\n\n# # Train the model\n# for epoch in range(EPOCHS):\n#     epoch_loss = 0\n#     model.train()\n\n#     for batch_idx, (src, trg, src_len, trg_len) in enumerate(train_loader):\n#         #print(batch_idx)\n#         src = src.permute(1, 0)  # swapping the dimensions of src tensor\n#         trg = trg.permute(1, 0)  # swapping the dimensions of trg tensor\n\n#         src = src.to(device)\n#         trg = trg.to(device)\n\n#         optimizer.zero_grad()\n\n#         output = model(src, trg, TEACHER_FORCING_RATIO)\n\n#         # Ignore the first element of the output, which is initialized as all zeros\n#         # since we use it to store the output for the start-of-sequence token\n#         #print(output.shape[2])\n\n#         output = output[1:].reshape(-1, output.shape[2])\n#         #print(output.shape)\n#         #print(trg.shape)\n#         trg = trg[1:].reshape(-1)\n\n#         loss = criterion(output, trg)\n#         loss.backward()\n\n#         optimizer.step()\n\n#         epoch_loss += (loss.item())\n\n#         if batch_idx % 1000 == 0:\n#             print(f\"Epoch: {epoch}, Batch: {batch_idx}, Training...\")\n\n#     # Calculate word-level accuracy after every epoch\n#     val_acc,val_loss = calculate_word_level_accuracy(model,t_idx_to_char,val_loader,criterion)\n\n#     print(f\"Epoch: {epoch}, Loss: {epoch_loss / (len(train_loader))}, Val Acc: {val_acc}, Val loss: {val_loss}\")\n#     #wandb.log({'epoch': epoch, 'loss': loss.item(), 'test_acc': test_acc,'train_acc': train_acc,'val_acc': val_acc})\n\n\n# # Save best model\n# best_model_path = 'best_model_vanillaSeq2Seq.pth'\n# torch.save(model.state_dict(), best_model_path)\n# print(f\"Best model saved to {best_model_path}\")","metadata":{"id":"1ttOezGyGOgF","outputId":"5b6009d1-a1d6-4c5f-a538-409abc64fd30","execution":{"iopub.status.busy":"2024-05-08T21:32:05.342087Z","iopub.execute_input":"2024-05-08T21:32:05.342362Z","iopub.status.idle":"2024-05-08T21:32:05.356008Z","shell.execute_reply.started":"2024-05-08T21:32:05.342340Z","shell.execute_reply":"2024-05-08T21:32:05.355136Z"},"editable":false,"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"#val_acc,val_loss,c_trg,c_src,c_pred,i_trg,i_src,i_pred = calculate_word_level_accuracy1(model,t_idx_to_char,s_idx_to_char,test_loader,criterion)","metadata":{"id":"pc8CgAudsUZw","outputId":"b95bb07a-f497-450a-8d68-19648fb6eac4","execution":{"iopub.status.busy":"2024-05-08T21:32:05.357065Z","iopub.execute_input":"2024-05-08T21:32:05.357697Z","iopub.status.idle":"2024-05-08T21:32:05.368888Z","shell.execute_reply.started":"2024-05-08T21:32:05.357665Z","shell.execute_reply":"2024-05-08T21:32:05.368050Z"},"editable":false,"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# print(c_trg)\n# print(c_src)\n# print(c_pred)\n# import csv\n# def save_to_csv(src_list, trg_list, pred_list, file_name):\n#     rows = zip(src_list, trg_list, pred_list)\n\n#     with open(file_name, mode='w', newline='') as file:\n#         writer = csv.writer(file)\n#         writer.writerow(['English', 'Target', 'Predicted'])\n#         writer.writerows(rows)\n\n# save_to_csv(c_src,c_trg,c_pred,'correct_predictions.csv')\n# save_to_csv(i_src,i_trg,i_pred,'incorrect_predictions.csv')\n","metadata":{"id":"pbOnZb9bsYEn","outputId":"35495386-125d-4562-f661-954873d52f8e","execution":{"iopub.status.busy":"2024-05-08T21:32:05.369921Z","iopub.execute_input":"2024-05-08T21:32:05.370712Z","iopub.status.idle":"2024-05-08T21:32:05.382715Z","shell.execute_reply.started":"2024-05-08T21:32:05.370682Z","shell.execute_reply":"2024-05-08T21:32:05.381868Z"},"editable":false,"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"from signal import signal,SIGPIPE, SIG_DFL\nsignal(SIGPIPE,SIG_DFL)\n!pip install wandb -qU\nimport wandb\n!wandb login fbf80504ccef17f5f3b05723be7ea4caff805164","metadata":{"id":"hjo4OyZ3UyZa","outputId":"5266d04b-d08b-48f2-a7c0-fd9f08e72df8","execution":{"iopub.status.busy":"2024-05-08T21:32:05.383627Z","iopub.execute_input":"2024-05-08T21:32:05.383863Z","iopub.status.idle":"2024-05-08T21:32:26.010329Z","shell.execute_reply.started":"2024-05-08T21:32:05.383842Z","shell.execute_reply":"2024-05-08T21:32:26.009016Z"},"editable":false,"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# # Load the CSV file\n# import pandas as pd\n# c_dataframe = pd.read_csv(\"/content/correct_predictions.csv\")\n# table = wandb.Table(dataframe=c_dataframe)\n\n# # Add the table to an Artifact to increase the row\n# # limit to 200000 and make it easier to reuse\n# c_table_artifact = wandb.Artifact(\n#     \"correct_predictions_vanilla\",\n#     type=\"dataset\"\n#     )\n# c_table_artifact.add(table, \"Correct_predictions\")\n\n# # Log the raw csv file within an artifact to preserve our data\n# c_table_artifact.add_file(\"/content/correct_predictions.csv\")\n\n# # Display as a table\n\n\n# run = wandb.init(project='Assignment_3_DL')\n\n# # Log the table to visualize with a run...\n# run.log({\"Vanilla_correct_predictions_table\": table})\n\n# # and Log as an Artifact to increase the available row limit!\n# run.log_artifact(c_table_artifact)\n\n","metadata":{"id":"blBCql1puVLK","execution":{"iopub.status.busy":"2024-05-08T21:32:26.011985Z","iopub.execute_input":"2024-05-08T21:32:26.012312Z","iopub.status.idle":"2024-05-08T21:32:26.017822Z","shell.execute_reply.started":"2024-05-08T21:32:26.012280Z","shell.execute_reply":"2024-05-08T21:32:26.016791Z"},"editable":false,"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"\n# # Load the CSV file\n# i_dataframe = pd.read_csv(\"/content/incorrect_predictions.csv\")\n# i_table = wandb.Table(dataframe=i_dataframe)\n\n# # Add the table to an Artifact to increase the row\n# # limit to 200000 and make it easier to reuse\n# i_table_artifact = wandb.Artifact(\n#     \"incorrect_predictions_vanilla\",\n#     type=\"dataset\"\n#     )\n# i_table_artifact.add(i_table, \"Incorrect_predictions\")\n\n# # Log the raw csv file within an artifact to preserve our data\n# i_table_artifact.add_file(\"/content/incorrect_predictions.csv\")\n\n# # Display as a table\n\n\n# run = wandb.init(project='\"Assignment_3_DL\"')\n\n# # Log the table to visualize with a run...\n# run.log({\"Vanilla_incorrect_predictions_table\": i_table})\n\n# # and Log as an Artifact to increase the available row limit!\n# run.log_artifact(i_table_artifact)\n\n","metadata":{"id":"obnBUXicxH98","execution":{"iopub.status.busy":"2024-05-08T21:32:26.019565Z","iopub.execute_input":"2024-05-08T21:32:26.020126Z","iopub.status.idle":"2024-05-08T21:32:26.037933Z","shell.execute_reply.started":"2024-05-08T21:32:26.020093Z","shell.execute_reply":"2024-05-08T21:32:26.036926Z"},"editable":false,"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# wandb sweeps\n\nsweep_config= {\n    \"name\" : \"Assignment_3_DL\",\n    \"method\" : \"bayes\",\n    'metric': {\n        'name': 'val_acc',\n        'goal': 'maximize'\n    },\n    'parameters' : {\n        'cell_type' : { 'values' : ['lstm','gru','rnn'] },\n        'dropout' : { 'values' : [0,0.1,0.2,0.5]},\n        'embedding_size' : {'values' : [64,128,256,512]},\n        'num_layers' : {'values' : [1]},\n        'batch_size' : {'values' : [32,64,128]},\n        'hidden_size' : {'values' : [128,256,512]},\n        'bidirectional' : {'values' : [True ,False]},\n        'learning_rate':{\n            \"values\": [0.001,0.002,0.0001,0.0002]\n        },\n        'optim':{\n            \"values\": ['adam','nadam']\n        },\n        'teacher_forcing':{\"values\":[0.2,0.5,0.7]}\n    }\n}\n\n\n\ndef train():\n    wandb.init()\n\n    c= wandb.config\n    name = \"cell_type_\"+str(c.cell_type)+\"_num_layers_\"+str(c.num_layers)+\"_dp_\"+str(c.dropout)+\"_bidir_\"+str(c.bidirectional)+\"_lr_\"+str(c.learning_rate)+\"_bs_\"+str(c.batch_size)\n    wandb.run.name=name\n\n    # Retrieve the hyperparameters from the config\n    ct=c.cell_type\n    dp = c.dropout\n    em=c.embedding_size\n    #nlayer=c.num_layers\n    nlayer=3\n    bs = c.batch_size\n    hs=c.hidden_size\n#     bidir = c.bidirectional\n    bidir = True\n    lr = c.learning_rate\n    opt= c.optim\n    epochs = 25\n    #tf=c.teacher_forcing\n    tf=0.7\n    trg_pad_idx=0\n\n\n\n    INPUT_DIM = 29\n    OUTPUT_DIM = 67\n\n\n  # Load the dataset\n    # train_loader,test_loader,val_loader,idx_to_char,s_idx_to_char=load_data(bs)\n    train_loader,test_loader,val_loader,idx_to_char,s_idx_to_char=load_data(bs)\n\n  #print(\"data loaded ====================================================\")\n\n\n\n\n    # # Define hyperparameters\n# INPUT_DIM = 29\n# OUTPUT_DIM = 67\n# embedding_size=512\n# HIDDEN_DIM = 512\n# NUM_LAYERS = 3\n# CELL_TYPE = 'gru'\n# BATCH_SIZE = 128\n# LEARNING_RATE = 0.0002\n# TEACHER_FORCING_RATIO = 0.7\n# EPOCHS = 25\n\n# dropout=0.1\n# bidirectional=True\n# opt='adam'\n\n    \n    \n\n  # Instantiate the Encoder and Decoder models\n    encoder = Encoder(INPUT_DIM,em,hs,nlayer,True,ct,dp).to(device)\n    decoder = Decoder(OUTPUT_DIM,em,hs,nlayer,True,ct,dp).to(device)\n\n  # Instantiate the Seq2Seq model with the Encoder and Decoder models\n    model = Seq2Seq(encoder,decoder,ct,True).to(device)\n  #print(\"model ini==============================================================\")\n\n  # Define the loss function and optimizer\n    criterion = nn.CrossEntropyLoss()\n    if opt == \"adam\":\n          optimizer = optim.Adam(model.parameters(),lr=lr)\n    elif opt == \"nadam\":\n          optimizer= optim.NAdam(model.parameters(),lr=lr)\n\n  # Train Network\n    for epoch in range(epochs):\n        epoch_loss = 0\n        model.train()\n\n        for batch_idx, (src, trg, src_len, trg_len) in enumerate(train_loader):\n            src = src.permute(1, 0)  # swapping the dimensions of src tensor\n            trg = trg.permute(1, 0)  # swapping the dimensions of trg tensor\n\n            src = src.to(device)\n            trg = trg.to(device)\n            #print(\"done\")\n            optimizer.zero_grad()\n            #print(\"doe\")\n            output = model(src,trg,tf)\n            #print(\"doe\")\n\n            # Ignore the first element of the output, which is initialized as all zeros\n            # since we use it to store the output for the start-of-sequence token\n            #print(output.shape[2])\n\n            output = output[1:].reshape(-1, output.shape[2])\n            #print(output.shape)\n            #print(trg.shape)\n            trg = trg[1:].reshape(-1)\n\n            loss = criterion(output, trg)\n            loss.backward()\n\n            optimizer.step()\n\n            epoch_loss += loss.item()\n\n            if batch_idx % 1000 == 0:\n                print(f\"Epoch: {epoch}, Batch: {batch_idx} , Training..\")\n\n        # Calculate word-level accuracy after every epoch\n        train_acc ,train_loss= calculate_word_level_accuracy(model,idx_to_char, train_loader,criterion)\n        val_acc,val_loss = calculate_word_level_accuracy(model,idx_to_char, val_loader, criterion)\n        test_acc,test_loss = calculate_word_level_accuracy(model,idx_to_char, test_loader, criterion)\n\n        print(f\"Epoch: {epoch}, Loss: {epoch_loss / len(train_loader)}, Train Acc: {train_acc}, Val Acc: {val_acc}\")\n\n\n        # Log the metrics to WandB\n        wandb.log({'epoch': epochs,'train_acc': 1.4*train_acc, 'train_loss': loss.item(), 'test_acc': test_acc*1.4, 'val_acc': val_acc*1.4,'test_loss': test_loss,'val_loss': val_loss})\n    # Save the best model\n    wandb.run.save()\n    wandb.run.finish()\n    return\n\n","metadata":{"id":"h9lJPg3DjBkT","execution":{"iopub.status.busy":"2024-05-08T21:32:26.039602Z","iopub.execute_input":"2024-05-08T21:32:26.039923Z","iopub.status.idle":"2024-05-08T21:32:26.061331Z","shell.execute_reply.started":"2024-05-08T21:32:26.039898Z","shell.execute_reply":"2024-05-08T21:32:26.060453Z"},"editable":false,"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# final train\n# Initialize the WandB sweep\n# sweep_id = wandb.sweep(sweep_config, project='Assignment_3_DL')\nsweep_id = wandb.sweep(sweep_config, entity='cs23m030', project=\"Assignment_3_DL\")\n# wandb.agent( function=train,count=1, project='Assignment_3_DL')\nwandb.agent(sweep_id, function=train,count=20)\n#wandb.agent(sweep_id, function=train,count=10)\n#wandb.agent(sweep_id, function=train,count=10)\n\n","metadata":{"id":"lhJ1jFFCjBxT","outputId":"d114fa50-899d-4813-9a48-af87d855e7b6","execution":{"iopub.status.busy":"2024-05-08T21:32:26.062389Z","iopub.execute_input":"2024-05-08T21:32:26.062641Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Create sweep with ID: vl4pvaku\nSweep URL: https://wandb.ai/cs23m030/Assignment_3_DL/sweeps/vl4pvaku\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: askapmqd with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbidirectional: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: lstm\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptim: nadam\n\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs23m030\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240508_213230-askapmqd</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/cs23m030/Assignment_3_DL/runs/askapmqd' target=\"_blank\">misunderstood-sweep-1</a></strong> to <a href='https://wandb.ai/cs23m030/Assignment_3_DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cs23m030/Assignment_3_DL/sweeps/vl4pvaku' target=\"_blank\">https://wandb.ai/cs23m030/Assignment_3_DL/sweeps/vl4pvaku</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/cs23m030/Assignment_3_DL' target=\"_blank\">https://wandb.ai/cs23m030/Assignment_3_DL</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/cs23m030/Assignment_3_DL/sweeps/vl4pvaku' target=\"_blank\">https://wandb.ai/cs23m030/Assignment_3_DL/sweeps/vl4pvaku</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/cs23m030/Assignment_3_DL/runs/askapmqd' target=\"_blank\">https://wandb.ai/cs23m030/Assignment_3_DL/runs/askapmqd</a>"},"metadata":{}},{"name":"stdout","text":"Epoch: 0, Batch: 0 , Training..\nEpoch: 0, Batch: 1000 , Training..\nTotal 51200\nCorrect 4\nTotal 4096\nCorrect 1\nTotal 4096\nCorrect 1\nEpoch: 0, Loss: 2.0253111638128756, Train Acc: 0.0078125, Val Acc: 0.0244140625\nEpoch: 1, Batch: 0 , Training..\nEpoch: 1, Batch: 1000 , Training..\nTotal 51200\nCorrect 1\nTotal 4096\nCorrect 0\nTotal 4096\nCorrect 2\nEpoch: 1, Loss: 1.4611314997076987, Train Acc: 0.001953125, Val Acc: 0.0\nEpoch: 2, Batch: 0 , Training..\nEpoch: 2, Batch: 1000 , Training..\nTotal 51200\nCorrect 2\nTotal 4096\nCorrect 0\nTotal 4096\nCorrect 1\nEpoch: 2, Loss: 1.3441850817203522, Train Acc: 0.00390625, Val Acc: 0.0\nEpoch: 3, Batch: 0 , Training..\nEpoch: 3, Batch: 1000 , Training..\nTotal 51200\nCorrect 7\nTotal 4096\nCorrect 0\nTotal 4096\nCorrect 2\nEpoch: 3, Loss: 1.261503385156393, Train Acc: 0.013671875000000002, Val Acc: 0.0\nEpoch: 4, Batch: 0 , Training..\nEpoch: 4, Batch: 1000 , Training..\nTotal 51200\nCorrect 27\nTotal 4096\nCorrect 16\nTotal 4096\nCorrect 8\nEpoch: 4, Loss: 1.1824718049541116, Train Acc: 0.052734375, Val Acc: 0.390625\nEpoch: 5, Batch: 0 , Training..\nEpoch: 5, Batch: 1000 , Training..\nTotal 51200\nCorrect 115\nTotal 4096\nCorrect 34\nTotal 4096\nCorrect 34\nEpoch: 5, Loss: 1.1131143780052661, Train Acc: 0.22460937499999997, Val Acc: 0.830078125\nEpoch: 6, Batch: 0 , Training..\nEpoch: 6, Batch: 1000 , Training..\nTotal 51200\nCorrect 238\nTotal 4096\nCorrect 55\nTotal 4096\nCorrect 58\nEpoch: 6, Loss: 1.039248423203826, Train Acc: 0.46484375, Val Acc: 1.3427734375\nEpoch: 7, Batch: 0 , Training..\nEpoch: 7, Batch: 1000 , Training..\nTotal 51200\nCorrect 594\nTotal 4096\nCorrect 125\nTotal 4096\nCorrect 109\nEpoch: 7, Loss: 0.9588423343747854, Train Acc: 1.16015625, Val Acc: 3.0517578125\nEpoch: 8, Batch: 0 , Training..\nEpoch: 8, Batch: 1000 , Training..\nTotal 51200\nCorrect 1258\nTotal 4096\nCorrect 196\nTotal 4096\nCorrect 177\nEpoch: 8, Loss: 0.8921867194771766, Train Acc: 2.45703125, Val Acc: 4.78515625\nEpoch: 9, Batch: 0 , Training..\nEpoch: 9, Batch: 1000 , Training..\nTotal 51200\nCorrect 2099\nTotal 4096\nCorrect 269\nTotal 4096\nCorrect 258\nEpoch: 9, Loss: 0.8364869271218777, Train Acc: 4.099609375, Val Acc: 6.5673828125\nEpoch: 10, Batch: 0 , Training..\nEpoch: 10, Batch: 1000 , Training..\nTotal 51200\nCorrect 2712\nTotal 4096\nCorrect 334\nTotal 4096\nCorrect 308\nEpoch: 10, Loss: 0.7941623277589679, Train Acc: 5.296875, Val Acc: 8.154296875\nEpoch: 11, Batch: 0 , Training..\nEpoch: 11, Batch: 1000 , Training..\nTotal 51200\nCorrect 3505\nTotal 4096\nCorrect 396\nTotal 4096\nCorrect 358\nEpoch: 11, Loss: 0.7535762167721987, Train Acc: 6.845703124999999, Val Acc: 9.66796875\nEpoch: 12, Batch: 0 , Training..\nEpoch: 12, Batch: 1000 , Training..\nTotal 51200\nCorrect 3893\nTotal 4096\nCorrect 432\nTotal 4096\nCorrect 404\nEpoch: 12, Loss: 0.7206844431720674, Train Acc: 7.603515625, Val Acc: 10.546875\nEpoch: 13, Batch: 0 , Training..\nEpoch: 13, Batch: 1000 , Training..\nTotal 51200\nCorrect 5458\nTotal 4096\nCorrect 585\nTotal 4096\nCorrect 542\nEpoch: 13, Loss: 0.6947901790961624, Train Acc: 10.66015625, Val Acc: 14.2822265625\nEpoch: 14, Batch: 0 , Training..\nEpoch: 14, Batch: 1000 , Training..\nTotal 51200\nCorrect 6004\nTotal 4096\nCorrect 623\nTotal 4096\nCorrect 537\nEpoch: 14, Loss: 0.6678617776185274, Train Acc: 11.7265625, Val Acc: 15.2099609375\nEpoch: 15, Batch: 0 , Training..\nEpoch: 15, Batch: 1000 , Training..\nTotal 51200\nCorrect 6881\nTotal 4096\nCorrect 712\nTotal 4096\nCorrect 628\nEpoch: 15, Loss: 0.6494336996041238, Train Acc: 13.439453125, Val Acc: 17.3828125\nEpoch: 16, Batch: 0 , Training..\nEpoch: 16, Batch: 1000 , Training..\nTotal 51200\nCorrect 7193\nTotal 4096\nCorrect 690\nTotal 4096\nCorrect 656\nEpoch: 16, Loss: 0.631986096277833, Train Acc: 14.048828125000002, Val Acc: 16.845703125\nEpoch: 17, Batch: 0 , Training..\nEpoch: 17, Batch: 1000 , Training..\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"id":"ivKTzXOlGmzi","editable":false},"execution_count":null,"outputs":[]}]}